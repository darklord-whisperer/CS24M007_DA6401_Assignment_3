{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4o61iFCAsGx",
        "outputId": "531c74f3-d2ed-457c-8267-462f9398ecd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n",
            "\u001b[K     |████████████████████████████████| 497.5 MB 19 kB/s \n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 37.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 462 kB 46.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 11.2 MB/s \n",
            "\u001b[?25h  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# importing the libraries and gpt2\n",
        "import os\n",
        "import pandas as pd\n",
        "# import tensorflow as tf\n",
        "%tensorflow_version 1.x\n",
        "\n",
        "try:\n",
        "  import gpt_2_simple as gpt2\n",
        "except:\n",
        "  !pip3 -q install gpt-2-simple\n",
        "  import gpt_2_simple as gpt2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the dataset.\n",
        "master_url = \"https://data.world/datasets/lyrics\"\n",
        "\n",
        "dfs = []\n",
        "link = ('https://raw.githubusercontent.com/ThomasVrancken/lyrics_generation/master/songdata_{}.csv' )\n",
        "\n",
        "for i in range(4):\n",
        "  dfs.append(pd.read_csv(link.format(i)))\n",
        "\n",
        "df = pd.concat(dfs).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "zu6Mc6zSE4NJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing the dataset in lyrics.csv file_name\n",
        "\n",
        "if not os.path.exists('content'):\n",
        "    os.makedirs('content')\n",
        "  \n",
        "pd.DataFrame({\"lyrics\": df['text']}).to_csv(os.path.join('content', 'lyrics.csv'), index=False)\n",
        "\n",
        "# https://blog.ml6.eu/gpt-2-artificial-intelligence-song-generator-lets-get-groovy-3e7c1f55030f"
      ],
      "metadata": {
        "id": "kJEYOOGJFNvp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name in [\"124M\"]:  # Choose from [\"124M\",\"355M\",\"774M\"]\n",
        "    gpt2.download_gpt2(model_name=model_name)   # model is saved into current directory under /models/124M/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBYftxckFfks",
        "outputId": "9d127f1f-8d1b-4d02-8b71-56a2e4a313fe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 668Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 2.40Mit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 602Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:17, 27.9Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 826Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 3.66Mit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 3.10Mit/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "learning_rate = 0.0001\n",
        "optimizer = 'adam'\n",
        "model_name = \"124M\"\n",
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "# Start a session\n",
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "# Finetune the model.\n",
        "gpt2.finetune(sess,\n",
        "              'content/lyrics.csv',\n",
        "              model_name=model_name,    #Subfolder within checkpoint to save the model\n",
        "              sample_every=500,\n",
        "              save_every=500,\n",
        "              print_every=5,\n",
        "              learning_rate=learning_rate,\n",
        "              restore_from='latest',\n",
        "              steps=1000    # Max number of steps.\n",
        "              )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqA4m0bHFqhd",
        "outputId": "c75f6218-87e4-4cfe-f38d-a132acafb000"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset has 23199731 tokens\n",
            "Training...\n",
            "[5 | 16.32] loss=2.43 avg=2.43\n",
            "[10 | 27.25] loss=2.42 avg=2.42\n",
            "[15 | 38.29] loss=2.25 avg=2.37\n",
            "[20 | 49.45] loss=2.18 avg=2.32\n",
            "[25 | 60.73] loss=2.02 avg=2.26\n",
            "[30 | 72.13] loss=1.83 avg=2.18\n",
            "[35 | 83.63] loss=2.10 avg=2.17\n",
            "[40 | 95.20] loss=2.03 avg=2.15\n",
            "[45 | 106.65] loss=2.11 avg=2.15\n",
            "[50 | 118.04] loss=2.39 avg=2.17\n",
            "[55 | 129.40] loss=2.03 avg=2.16\n",
            "[60 | 140.76] loss=1.99 avg=2.14\n",
            "[65 | 152.16] loss=2.02 avg=2.13\n",
            "[70 | 163.61] loss=1.77 avg=2.11\n",
            "[75 | 175.06] loss=2.18 avg=2.11\n",
            "[80 | 186.49] loss=1.98 avg=2.10\n",
            "[85 | 197.93] loss=1.90 avg=2.09\n",
            "[90 | 209.35] loss=1.66 avg=2.06\n",
            "[95 | 220.76] loss=2.26 avg=2.08\n",
            "[100 | 232.17] loss=2.07 avg=2.07\n",
            "[105 | 243.59] loss=1.88 avg=2.06\n",
            "[110 | 255.01] loss=2.33 avg=2.08\n",
            "[115 | 266.44] loss=2.26 avg=2.09\n",
            "[120 | 277.88] loss=2.02 avg=2.08\n",
            "[125 | 289.32] loss=1.82 avg=2.07\n",
            "[130 | 300.76] loss=2.02 avg=2.07\n",
            "[135 | 312.20] loss=2.09 avg=2.07\n",
            "[140 | 323.63] loss=1.98 avg=2.07\n",
            "[145 | 335.09] loss=1.96 avg=2.06\n",
            "[150 | 346.54] loss=2.28 avg=2.07\n",
            "[155 | 358.00] loss=1.97 avg=2.07\n",
            "[160 | 369.45] loss=2.00 avg=2.06\n",
            "[165 | 380.90] loss=1.90 avg=2.06\n",
            "[170 | 392.34] loss=1.87 avg=2.05\n",
            "[175 | 403.77] loss=2.25 avg=2.06\n",
            "[180 | 415.22] loss=2.10 avg=2.06\n",
            "[185 | 426.66] loss=1.85 avg=2.05\n",
            "[190 | 438.09] loss=2.10 avg=2.05\n",
            "[195 | 449.53] loss=1.63 avg=2.04\n",
            "[200 | 460.95] loss=2.07 avg=2.04\n",
            "[205 | 472.38] loss=2.17 avg=2.05\n",
            "[210 | 483.83] loss=2.50 avg=2.06\n",
            "[215 | 495.25] loss=2.11 avg=2.06\n",
            "[220 | 506.66] loss=1.92 avg=2.06\n",
            "[225 | 518.10] loss=1.93 avg=2.05\n",
            "[230 | 529.54] loss=1.98 avg=2.05\n",
            "[235 | 540.97] loss=1.88 avg=2.05\n",
            "[240 | 552.39] loss=2.37 avg=2.06\n",
            "[245 | 563.83] loss=2.50 avg=2.07\n",
            "[250 | 575.27] loss=1.77 avg=2.06\n",
            "[255 | 586.74] loss=1.99 avg=2.06\n",
            "[260 | 598.19] loss=1.87 avg=2.05\n",
            "[265 | 609.64] loss=2.11 avg=2.05\n",
            "[270 | 621.07] loss=2.04 avg=2.05\n",
            "[275 | 632.51] loss=1.66 avg=2.05\n",
            "[280 | 643.96] loss=2.08 avg=2.05\n",
            "[285 | 655.40] loss=2.24 avg=2.05\n",
            "[290 | 666.84] loss=1.94 avg=2.05\n",
            "[295 | 678.28] loss=1.94 avg=2.05\n",
            "[300 | 689.73] loss=2.14 avg=2.05\n",
            "[305 | 701.19] loss=1.93 avg=2.05\n",
            "[310 | 712.64] loss=2.15 avg=2.05\n",
            "[315 | 724.10] loss=1.99 avg=2.05\n",
            "[320 | 735.54] loss=2.13 avg=2.05\n",
            "[325 | 746.98] loss=1.89 avg=2.04\n",
            "[330 | 758.41] loss=2.00 avg=2.04\n",
            "[335 | 769.87] loss=2.07 avg=2.04\n",
            "[340 | 781.33] loss=1.87 avg=2.04\n",
            "[345 | 792.79] loss=2.11 avg=2.04\n",
            "[350 | 804.24] loss=2.09 avg=2.04\n",
            "[355 | 815.69] loss=2.00 avg=2.04\n",
            "[360 | 827.13] loss=2.01 avg=2.04\n",
            "[365 | 838.55] loss=1.99 avg=2.04\n",
            "[370 | 849.99] loss=1.84 avg=2.04\n",
            "[375 | 861.43] loss=1.71 avg=2.03\n",
            "[380 | 872.86] loss=1.94 avg=2.03\n",
            "[385 | 884.29] loss=1.59 avg=2.02\n",
            "[390 | 895.71] loss=2.01 avg=2.02\n",
            "[395 | 907.13] loss=1.97 avg=2.02\n",
            "[400 | 918.55] loss=2.24 avg=2.02\n",
            "[405 | 929.98] loss=2.04 avg=2.02\n",
            "[410 | 941.42] loss=1.87 avg=2.02\n",
            "[415 | 952.85] loss=1.97 avg=2.02\n",
            "[420 | 964.30] loss=2.58 avg=2.03\n",
            "[425 | 975.73] loss=2.14 avg=2.03\n",
            "[430 | 987.17] loss=1.76 avg=2.03\n",
            "[435 | 998.59] loss=1.97 avg=2.03\n",
            "[440 | 1010.02] loss=1.93 avg=2.02\n",
            "[445 | 1021.46] loss=1.80 avg=2.02\n",
            "[450 | 1032.90] loss=2.08 avg=2.02\n",
            "[455 | 1044.34] loss=1.91 avg=2.02\n",
            "[460 | 1055.78] loss=1.82 avg=2.02\n",
            "[465 | 1067.22] loss=1.89 avg=2.01\n",
            "[470 | 1078.65] loss=2.02 avg=2.01\n",
            "[475 | 1090.15] loss=1.97 avg=2.01\n",
            "[480 | 1101.70] loss=2.06 avg=2.01\n",
            "[485 | 1113.18] loss=1.98 avg=2.01\n",
            "[490 | 1124.61] loss=1.79 avg=2.01\n",
            "[495 | 1136.06] loss=1.76 avg=2.01\n",
            "[500 | 1147.50] loss=1.92 avg=2.00\n",
            "Saving checkpoint/run1/model-500\n",
            "======== SAMPLE 1 ========\n",
            "  \n",
            "  \n",
            "The one we want is something deeper  \n",
            "When will we ever leave?  \n",
            "When will we ever forget?  \n",
            "  \n",
            "You've got to be strong.  \n",
            "To know we can't win.  \n",
            "To be honest,  \n",
            "  \n",
            "The feeling we want is something deeper\n",
            "\n",
            "<|endoftext|>\n",
            "<|startoftext|>This thing is so complicated  \n",
            "It's gotta come to an end  \n",
            "  \n",
            "What's the point?  \n",
            "We're still trying to understand  \n",
            "If nothing I just won't let you in.  \n",
            "  \n",
            "Well, I don't know what's wrong with the world  \n",
            "We know there's a world we can't stop from changing  \n",
            "And there's a world that needs to change  \n",
            "Oh, we live in  \n",
            "This place I know so well, it's time to  \n",
            "  \n",
            "This thing is so complicated  \n",
            "It's gotta come to an end  \n",
            "  \n",
            "What's the point?  \n",
            "We're still trying to understand  \n",
            "If nothing I just won't let you in.  \n",
            "  \n",
            "I don't see what's wrong with the world  \n",
            "We know there's a world we can't stop from changing  \n",
            "And there's a world that needs to change  \n",
            "Oh, we live in  \n",
            "This place I know so well, it's time to  \n",
            "  \n",
            "This thing is so complicated  \n",
            "It's gotta come to an end  \n",
            "  \n",
            "What's the point?  \n",
            "We're still trying to understand  \n",
            "If nothing I just won't let you in.  \n",
            "Oh, we live in  \n",
            "This place I know so well, it's time to  \n",
            "  \n",
            "This thing?  \n",
            "It's got to come to an end  \n",
            "I won't let you in  \n",
            "But what's the point?  \n",
            "We are trying to change  \n",
            "It's gonna come to an end  \n",
            "  \n",
            "These problems are not the same  \n",
            "  \n",
            "If we can't see, what's the point?  \n",
            "  \n",
            "What's the point?  \n",
            "We're still trying to understand  \n",
            "If nothing I just won't let you in.  \n",
            "  \n",
            "I don't see what's wrong with the world  \n",
            "We know there's a world we can't stop from changing  \n",
            "And there's a world that needs to change\n",
            "\n",
            "<|endoftext|>\n",
            "<|startoftext|>When people get together  \n",
            "They're really nice  \n",
            "They're kind enough  \n",
            "To have a good time  \n",
            "  \n",
            "I want to be the one  \n",
            "Who thinks he's gonna do  \n",
            "Better than me  \n",
            "  \n",
            "It's so bad when I say  \n",
            "I want to be all alone  \n",
            "And tell all the people  \n",
            "They're not there to stay  \n",
            "But you're the one  \n",
            "Don't you know  \n",
            "  \n",
            "The last time I went  \n",
            "I never gave a damn  \n",
            "Cause they would have liked for an  \n",
            "Baby to be with me  \n",
            "  \n",
            "I don't want to be around you all the  \n",
            "Time's for love  \n",
            "  \n",
            "How did I fall asleep  \n",
            "I don't know  \n",
            "Do you know  \n",
            "Why I'm gone  \n",
            "  \n",
            "I want to be the one  \n",
            "Who thinks he's gonna do  \n",
            "Better than me  \n",
            "  \n",
            "It's so bad when I say  \n",
            "I'm always the one  \n",
            "Who thinks he's gonna do  \n",
            "Better than me  \n",
            "  \n",
            "It's so bad when I say  \n",
            "I want to be all alone  \n",
            "And it's so bad when I say  \n",
            "I want to be all alone  \n",
            "  \n",
            "It's so bad when I say  \n",
            "I want to be all alone  \n",
            "And I want\n",
            "\n",
            "<|endoftext|>\n",
            "<|startoftext|>You were so strong and I was so cold  \n",
            "You were so brave and you were so bad  \n",
            "Then my heart was the same  \n",
            "When you were young and that's how we all knew  \n",
            "You were so strong and I was so scared  \n",
            "You came before me and I was with  \n",
            "We got to be together  \n",
            "And we got to be alright  \n",
            "I couldn't give no one for no time \n",
            "\n",
            "[505 | 1178.31] loss=2.02 avg=2.01\n",
            "[510 | 1189.92] loss=2.02 avg=2.01\n",
            "[515 | 1201.46] loss=1.72 avg=2.00\n",
            "[520 | 1212.93] loss=1.82 avg=2.00\n",
            "[525 | 1224.35] loss=2.03 avg=2.00\n",
            "[530 | 1235.77] loss=1.87 avg=2.00\n",
            "[535 | 1247.19] loss=1.60 avg=1.99\n",
            "[540 | 1258.67] loss=1.99 avg=1.99\n",
            "[545 | 1270.16] loss=2.07 avg=1.99\n",
            "[550 | 1281.64] loss=1.91 avg=1.99\n",
            "[555 | 1293.12] loss=1.98 avg=1.99\n",
            "[560 | 1304.58] loss=1.94 avg=1.99\n",
            "[565 | 1316.04] loss=1.79 avg=1.99\n",
            "[570 | 1327.48] loss=1.95 avg=1.99\n",
            "[575 | 1338.92] loss=1.71 avg=1.98\n",
            "[580 | 1350.36] loss=2.12 avg=1.98\n",
            "[585 | 1361.81] loss=2.17 avg=1.99\n",
            "[590 | 1373.25] loss=1.79 avg=1.98\n",
            "[595 | 1384.71] loss=1.78 avg=1.98\n",
            "[600 | 1396.14] loss=2.23 avg=1.98\n",
            "[605 | 1407.59] loss=2.00 avg=1.98\n",
            "[610 | 1419.05] loss=2.11 avg=1.99\n",
            "[615 | 1430.52] loss=2.12 avg=1.99\n",
            "[620 | 1441.99] loss=1.93 avg=1.99\n",
            "[625 | 1453.45] loss=2.13 avg=1.99\n",
            "[630 | 1464.91] loss=2.23 avg=1.99\n",
            "[635 | 1476.40] loss=2.03 avg=1.99\n",
            "[640 | 1487.89] loss=2.13 avg=2.00\n",
            "[645 | 1499.37] loss=2.07 avg=2.00\n",
            "[650 | 1510.85] loss=2.17 avg=2.00\n",
            "[655 | 1522.32] loss=2.05 avg=2.00\n",
            "[660 | 1533.80] loss=2.24 avg=2.00\n",
            "[665 | 1545.29] loss=2.16 avg=2.01\n",
            "[670 | 1556.76] loss=1.85 avg=2.00\n",
            "[675 | 1568.24] loss=2.16 avg=2.01\n",
            "[680 | 1579.73] loss=1.98 avg=2.00\n",
            "[685 | 1591.22] loss=1.91 avg=2.00\n",
            "[690 | 1602.69] loss=1.97 avg=2.00\n",
            "[695 | 1614.16] loss=2.21 avg=2.01\n",
            "[700 | 1625.62] loss=1.95 avg=2.00\n",
            "[705 | 1637.07] loss=1.83 avg=2.00\n",
            "[710 | 1648.54] loss=1.96 avg=2.00\n",
            "[715 | 1659.98] loss=1.93 avg=2.00\n",
            "[720 | 1671.46] loss=2.03 avg=2.00\n",
            "[725 | 1682.92] loss=2.00 avg=2.00\n",
            "[730 | 1694.38] loss=2.20 avg=2.00\n",
            "[735 | 1705.86] loss=1.93 avg=2.00\n",
            "[740 | 1717.35] loss=1.64 avg=2.00\n",
            "[745 | 1728.83] loss=2.09 avg=2.00\n",
            "[750 | 1740.30] loss=2.12 avg=2.00\n",
            "[755 | 1751.79] loss=1.84 avg=2.00\n",
            "[760 | 1763.26] loss=1.80 avg=2.00\n",
            "[765 | 1774.72] loss=2.02 avg=2.00\n",
            "[770 | 1786.21] loss=2.32 avg=2.00\n",
            "[775 | 1797.69] loss=1.86 avg=2.00\n",
            "[780 | 1809.15] loss=2.23 avg=2.00\n",
            "[785 | 1820.60] loss=1.93 avg=2.00\n",
            "[790 | 1832.07] loss=2.18 avg=2.00\n",
            "[795 | 1843.53] loss=1.97 avg=2.00\n",
            "[800 | 1854.98] loss=1.59 avg=2.00\n",
            "[805 | 1866.43] loss=2.31 avg=2.00\n",
            "[810 | 1877.88] loss=2.16 avg=2.00\n",
            "[815 | 1889.32] loss=1.81 avg=2.00\n",
            "[820 | 1900.75] loss=1.76 avg=2.00\n",
            "[825 | 1912.22] loss=2.11 avg=2.00\n",
            "[830 | 1923.68] loss=2.34 avg=2.00\n",
            "[835 | 1935.14] loss=2.01 avg=2.00\n",
            "[840 | 1946.61] loss=2.06 avg=2.00\n",
            "[845 | 1958.07] loss=1.61 avg=2.00\n",
            "[850 | 1969.54] loss=1.99 avg=2.00\n",
            "[855 | 1981.02] loss=1.93 avg=2.00\n",
            "[860 | 1992.51] loss=1.94 avg=2.00\n",
            "[865 | 2003.98] loss=2.00 avg=2.00\n",
            "[870 | 2015.45] loss=1.52 avg=1.99\n",
            "[875 | 2026.93] loss=1.89 avg=1.99\n",
            "[880 | 2038.41] loss=2.05 avg=1.99\n",
            "[885 | 2049.89] loss=1.46 avg=1.99\n",
            "[890 | 2061.35] loss=1.94 avg=1.98\n",
            "[895 | 2072.82] loss=2.15 avg=1.99\n",
            "[900 | 2084.28] loss=2.16 avg=1.99\n",
            "[905 | 2095.75] loss=2.20 avg=1.99\n",
            "[910 | 2107.21] loss=2.12 avg=1.99\n",
            "[915 | 2118.67] loss=2.28 avg=2.00\n",
            "[920 | 2130.12] loss=2.26 avg=2.00\n",
            "[925 | 2141.58] loss=1.71 avg=2.00\n",
            "[930 | 2153.04] loss=2.10 avg=2.00\n",
            "[935 | 2164.51] loss=1.63 avg=1.99\n",
            "[940 | 2175.98] loss=1.70 avg=1.99\n",
            "[945 | 2187.45] loss=2.06 avg=1.99\n",
            "[950 | 2198.94] loss=1.86 avg=1.99\n",
            "[955 | 2210.43] loss=1.88 avg=1.99\n",
            "[960 | 2221.91] loss=1.77 avg=1.98\n",
            "[965 | 2233.40] loss=1.79 avg=1.98\n",
            "[970 | 2244.88] loss=2.04 avg=1.98\n",
            "[975 | 2256.37] loss=1.89 avg=1.98\n",
            "[980 | 2267.85] loss=1.71 avg=1.98\n",
            "[985 | 2279.34] loss=2.01 avg=1.98\n",
            "[990 | 2290.81] loss=2.03 avg=1.98\n",
            "[995 | 2302.29] loss=1.85 avg=1.98\n",
            "[1000 | 2313.77] loss=2.16 avg=1.98\n",
            "Saving checkpoint/run1/model-1000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:1054: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the lyrics using the GPT2 model.\n",
        "lst_lyrics=gpt2.generate(\n",
        "    sess,\n",
        "    prefix=\"<|startoftext|>I love deep learning\",\n",
        "    nsamples=5, # Number of samples to generate.\n",
        "    temperature=0.95, # higher the temperature, the crazier the text\n",
        "    top_p=.95, # Nucleus sampling\n",
        "    top_k=20,   # Limits the generated guesses to the top k guesses\n",
        "    return_as_list=True,\n",
        "    truncate=\"<|endoftext|>\",\n",
        "    include_prefix=False\n",
        "    )\n",
        "\n",
        "for result in lst_lyrics:\n",
        "  print(result)"
      ],
      "metadata": {
        "id": "ZJxzLUfNGHJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<|startoftext|>I love deep learning  \n",
        "The things we learn  \n",
        "But we're no fools  \n",
        "I know it's hard to keep on learning  \n",
        "And I know it's gonna be hard sometimes to keep on learning \n",
        "\n",
        "I love deep learning, just like you did\n",
        "So, and only long enough feel it, don't be afraid to try\n",
        "Oh, you love deep learning\n",
        "And take it outside to deep learning\n",
        "Well, you love deep learning\n",
        "Because it's true, baby\n",
        "\n",
        "Oh well, baby\n",
        "Baby\n",
        "Baby\n",
        "\n",
        "I love learning to be strong  \n",
        "I love learning to stay strong  \n",
        "I know how to take a deep breath  \n",
        "  \n",
        "I know that it takes time  \n",
        "To learn from your mistakes  \n",
        "But I know how to learn  \n",
        "To learn from you  \n",
        "  \n",
        "I love learning to be strong  \n",
        "I love learning to be strong  \n",
        "I love learning to be strong  \n",
        "I know how to take a deep breath  \n",
        "  \n",
        "I know that it takes time  \n",
        "To learn from your mistakes  \n",
        "But I know how to learn  \n",
        "To learn from you  \n",
        "  \n",
        "Oh, oh, oh, oh, oh, oh  \n",
        "I love learning to be strong  \n",
        "I love learning to be strong  \n",
        "I love learning to be strong  \n",
        "I love learning to be strong  \n",
        "I know how to take a deep breath  \n",
        "  \n",
        "I know that it takes time  \n",
        "To learn from your mistakes  \n",
        "But I know how to learn  \n",
        "To learn from you  \n",
        "  \n",
        "Oh, oh, oh, oh, oh, oh  \n",
        "I love learning to be strong  \n",
        "I love learning to be strong  \n",
        "I love learning to be strong  \n",
        "I love learning to be strong  \n",
        "I love learning to be strong  \n",
        "I know how to take a deep breath  \n",
        "  \n",
        "I know that it takes time  \n",
        "To learn from your mistakes  \n",
        "Oh, oh, oh, oh, oh, oh  \n",
        "I love learning to be strong  \n",
        "I love learning to be strong  \n",
        "I love learning to be strong  \n",
        "\n",
        "I love deep learning  \n",
        "My brain is not the best  \n",
        "And I'm not the brightest  \n",
        "But I'm a genius  \n",
        "I got a vision of love - a brilliant vision  \n",
        "\n",
        "Deep learning is like a thousand dark ants waiting to swarm  \n",
        "When all the colonies are buried  \n",
        "In the mystery of their own death  \n",
        "Deep learning  \n",
        "Of every lover you've  \n",
        "I love deep learning  \n",
        "Of every lover you've  \n",
        "I love you deeper  \n",
        "There's a feeling that's just inside of me  \n",
        "I never went away  \n",
        "So deep learning  \n",
        "Of every lover you've  \n",
        "I love deep learning  \n",
        "Of every lover you've  \n",
        "I love you deeper\n",
        "\n",
        "Deep learning is all about the glass, the mistletoe  \n",
        "Deep learning is all about the shower, the golden tooth  \n",
        "Deep learning is all about the dust bowl  \n",
        "Deep learning is all about the ticket, the pumpkin patch  \n",
        "Deep learning is all about the jewelry, the tombstone  \n",
        "Deep learning is all about the guitar, the tuftle  \n",
        "Deep learning is all about the tee shirt, the  \n",
        "Deep learning is all about the eggplant  \n",
        "Deep learning is all about the songbook, the ruttle  \n",
        "Deep learning is all about the sheep and the lamb  \n",
        "Deep learning is all about the wandering frog  \n",
        "Deep learning is all about the things that are comin' up on you  \n",
        "Deep learning is all about the weather balloon  \n",
        "Deep learning is all about the string  \n",
        "Deep learning is all about the wine  \n",
        "Deep learning is all about the elevator  \n",
        "Deep learning is all about the house that everybody can see  \n",
        "Deep learning is all about the lawn  \n",
        "Deep learning is all about the redwoods  \n",
        "Deep learning is all about the sunbeams  \n",
        "Deep learning is all about the baubs  \n",
        "Deep learning is all about the birds in the tree  \n",
        "Deep learning is all about the alligator  \n",
        "Deep learning is all about the rubberstamps  \n",
        "Deep learning is all about the sail boat  \n",
        "Deep learning is all about the home train  \n",
        "Deep learning is all about the wheels of flight  \n",
        "Deep learning is all about the sunbeams  \n",
        "Deep learning is all about the lanes of wailing  \n",
        "Deep learning is all about the gunners and the war  \n",
        "Deep learning is all about the skies that are blue  \n",
        "Deep learning is all about the wagons that make the winter nights  \n",
        "Deep learning is all about the deep wind  \n",
        "Deep learning is all about the suckers  \n",
        "Deep learning is all about the gingerbreadmen  \n",
        "Deep learning is all about the winding sands  \n",
        "Deep learning is all about the wind and the straw trees  \n",
        "Deep learning is all about the world that's left behind  \n",
        "Deep learning is all about the jelly  \n",
        "Deep learning is all about the chicken soup, creamed rice and fried chicken  \n",
        "Deep learning is all about the sponges that make it look so bad  \n",
        "Deep learning is all about the moors in the hay and the chickens that flew away  \n",
        "Deep learning is all about the lapel stones \n",
        "\n",
        "Deep learning is like a thousand dark ants waiting to swarm  \n",
        "When all the colonies are buried  \n",
        "In the mystery of their own death  \n",
        "Deep learning  \n",
        "Of every lover you've  \n",
        "I love deep learning  \n",
        "Of every lover you've  \n",
        "I love you deeper  \n",
        "There's a feeling that's just inside of me  \n",
        "I never went away  \n",
        "So deep learning  \n",
        "Of every lover you've  \n",
        "I love deep learning  \n",
        "Of every lover you've  \n",
        "I love you deeper"
      ],
      "metadata": {
        "id": "h7lnqaBFn-63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  To zip the finetuned model.\n",
        "!zip -r /content/run_1000.zip /content/checkpoint/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1ftoOzNajtt",
        "outputId": "b06ed593-db23-462d-fb53-80832d47f8f1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/checkpoint/ (stored 0%)\n",
            "  adding: content/checkpoint/run1/ (stored 0%)\n",
            "  adding: content/checkpoint/run1/model-1000.data-00000-of-00001 (deflated 7%)\n",
            "  adding: content/checkpoint/run1/encoder.json (deflated 67%)\n",
            "  adding: content/checkpoint/run1/model-1000.meta (deflated 92%)\n",
            "  adding: content/checkpoint/run1/hparams.json (deflated 28%)\n",
            "  adding: content/checkpoint/run1/checkpoint (deflated 40%)\n",
            "  adding: content/checkpoint/run1/model-1000.index (deflated 62%)\n",
            "  adding: content/checkpoint/run1/counter (stored 0%)\n",
            "  adding: content/checkpoint/run1/vocab.bpe (deflated 53%)\n",
            "  adding: content/checkpoint/run1/events.out.tfevents.1651972253.becae0fd4ef0 (deflated 60%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To save the finetuned model\n",
        "from google.colab import files\n",
        "files.download(\"/content/run_1000.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Ijf-w4HhaYgi",
        "outputId": "0ff6b99f-5471-4e90-b2bd-30cb2a861d6f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_20943293-7957-4d1b-8ff0-63880979b91c\", \"run_1000.zip\", 463231821)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}